{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0: -500.0\n",
      "episode 10: -240.0\n",
      "episode 20: -285.0\n",
      "episode 30: -324.0\n",
      "episode 40: -500.0\n",
      "episode 50: -500.0\n",
      "episode 60: -500.0\n",
      "episode 70: -500.0\n",
      "episode 80: -500.0\n",
      "episode 90: -500.0\n",
      "episode 100: -500.0\n",
      "episode 110: -500.0\n",
      "episode 120: -500.0\n",
      "episode 130: -500.0\n",
      "episode 140: -500.0\n",
      "episode 150: -500.0\n",
      "episode 160: -500.0\n",
      "episode 170: -500.0\n",
      "episode 180: -500.0\n",
      "episode 190: -500.0\n",
      "episode 200: -500.0\n",
      "episode 210: -500.0\n",
      "episode 220: -500.0\n",
      "episode 230: -500.0\n",
      "episode 240: -500.0\n",
      "episode 250: -500.0\n",
      "episode 260: -500.0\n",
      "episode 270: -500.0\n",
      "episode 280: -500.0\n",
      "episode 290: -500.0\n",
      "episode 300: -500.0\n",
      "episode 310: -500.0\n",
      "episode 320: -500.0\n",
      "episode 330: -500.0\n",
      "episode 340: -500.0\n",
      "episode 350: -500.0\n",
      "episode 360: -500.0\n",
      "episode 370: -500.0\n",
      "episode 380: -500.0\n",
      "episode 390: -500.0\n",
      "episode 400: -500.0\n",
      "episode 410: -500.0\n",
      "episode 420: -500.0\n",
      "episode 430: -500.0\n",
      "episode 440: -500.0\n",
      "episode 450: -500.0\n",
      "episode 460: -500.0\n",
      "episode 470: -500.0\n",
      "episode 480: -500.0\n",
      "episode 490: -500.0\n",
      "episode 0: -500.0\n",
      "episode 10: -92.0\n",
      "episode 20: -110.0\n",
      "episode 30: -77.0\n",
      "episode 40: -88.0\n",
      "episode 50: -98.0\n",
      "episode 60: -85.0\n",
      "episode 70: -123.0\n",
      "episode 80: -96.0\n",
      "episode 90: -78.0\n",
      "episode 100: -95.0\n",
      "episode 110: -97.0\n",
      "episode 120: -64.0\n",
      "episode 130: -67.0\n",
      "episode 140: -73.0\n",
      "episode 150: -83.0\n",
      "episode 160: -106.0\n",
      "episode 170: -84.0\n",
      "episode 180: -136.0\n",
      "episode 190: -96.0\n",
      "episode 200: -500.0\n",
      "episode 210: -73.0\n",
      "episode 220: -73.0\n",
      "episode 230: -73.0\n",
      "episode 240: -95.0\n",
      "episode 250: -170.0\n",
      "episode 260: -113.0\n",
      "episode 270: -90.0\n",
      "episode 280: -64.0\n",
      "episode 290: -92.0\n",
      "episode 300: -87.0\n",
      "episode 310: -98.0\n",
      "episode 320: -77.0\n",
      "episode 330: -109.0\n",
      "episode 340: -170.0\n",
      "episode 350: -81.0\n",
      "episode 360: -90.0\n",
      "episode 370: -65.0\n",
      "episode 380: -87.0\n",
      "episode 390: -65.0\n",
      "episode 400: -79.0\n",
      "episode 410: -98.0\n",
      "episode 420: -101.0\n",
      "episode 430: -76.0\n",
      "episode 440: -82.0\n",
      "episode 450: -74.0\n",
      "episode 460: -96.0\n",
      "episode 470: -100.0\n",
      "episode 480: -72.0\n",
      "episode 490: -80.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch  \n",
    "import gym\n",
    "import numpy as np  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import obstacle_env\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# Constants\n",
    "GAMMA = 0.99\n",
    "init = True\n",
    "model_path = 'reinforce_weights.pt'\n",
    "save = False\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size=256):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_actions)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.softmax(self.linear2(x), dim=1)\n",
    "        return x \n",
    "    \n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, env, init, path, learning_rate=0.01):\n",
    "        self.env = env\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.policy_network = PolicyNetwork(self.env.observation_space.shape[0], self.env.action_space.n)\n",
    "        if not init:\n",
    "            self.policy_network.load_state_dict(torch.load(path))\n",
    "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=learning_rate)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        probs = self.policy_network.forward(Variable(state))\n",
    "        highest_prob_action = np.random.choice(self.num_actions, p=np.squeeze(probs.detach().numpy()))\n",
    "        log_prob = torch.log(probs.squeeze(0)[highest_prob_action])\n",
    "        return highest_prob_action, log_prob\n",
    "\n",
    "    def update_policy(self, rewards, log_probs):\n",
    "        discounted_rewards = []\n",
    "\n",
    "        for t in range(len(rewards)):\n",
    "            Gt = 0 \n",
    "            pw = 0\n",
    "            for r in rewards[t:]:\n",
    "                Gt = Gt + GAMMA**pw * r\n",
    "                pw = pw + 1\n",
    "            discounted_rewards.append(Gt)\n",
    "            \n",
    "        discounted_rewards = torch.FloatTensor(discounted_rewards)\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9) # normalize discounted rewards\n",
    "\n",
    "        policy_gradient = []\n",
    "        for log_prob, Gt in zip(log_probs, discounted_rewards):\n",
    "            policy_gradient.append(-log_prob * Gt)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        policy_gradient = torch.stack(policy_gradient).sum()\n",
    "        policy_gradient.backward()\n",
    "        if init:\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def train(self, max_episode=3000, max_step=200):\n",
    "        timestamps = []\n",
    "        time_start = time.time()\n",
    "        episode_rewards = []\n",
    "        for episode in range(max_episode):\n",
    "            state = env.reset()\n",
    "            log_probs = []\n",
    "            rewards = []\n",
    "            episode_reward = 0\n",
    "\n",
    "            for steps in range(max_step):\n",
    "                action, log_prob = self.get_action(state)\n",
    "                new_state, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                log_probs.append(log_prob)\n",
    "                rewards.append(reward)\n",
    "                episode_reward += reward\n",
    "                \n",
    "                if done or steps == max_step - 1:\n",
    "                    self.update_policy(rewards, log_probs)\n",
    "                    if episode % 10 == 0:\n",
    "                        print(\"episode \" + str(episode) + \": \" + str(episode_reward))\n",
    "\n",
    "                    break\n",
    "                \n",
    "                state = new_state\n",
    "            episode_rewards.append(episode_reward)\n",
    "            timestamps.append(time.time())\n",
    "        return time_start, timestamps, episode_rewards\n",
    "        \n",
    "\n",
    "\n",
    "def save_results(rewards, timestamps, seed, time_start, env_name, init, n_run, name='reinforce'):\n",
    "    run_dict = {'name': name, \n",
    "                'rewards': rewards,\n",
    "                'time_start': time_start, \n",
    "                'timestamps': timestamps,\n",
    "                'seed': seed,\n",
    "                'n_run': n_run,\n",
    "                'env_name': env_name}\n",
    "    if not init:\n",
    "        filename = 'run_time_%s_%s_%s.pickle' % (name, seed, 'pretrained')\n",
    "    else:\n",
    "        filename = 'run_time_%s_%s.pickle' % (name, seed)\n",
    "    with open(filename, 'wb') as handle:\n",
    "        pickle.dump(run_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    #env_name = 'CartPole-v0'\n",
    "    env_name = 'Acrobot-v1'\n",
    "    env = gym.make(env_name)\n",
    "    SEED = 1234\n",
    "    env.seed(SEED);\n",
    "    np.random.seed(SEED);\n",
    "    #env.render()\n",
    "    torch.manual_seed(SEED);\n",
    "    num_runs = 2\n",
    "    result_dict = {}\n",
    "    for run in range(num_runs):\n",
    "        agent = Agent(env, init, model_path)\n",
    "        time_start, timestamps, rewards = agent.train(500,500)\n",
    "        save_results(rewards, timestamps, SEED, time_start, env_name, init, run)\n",
    "        if save:\n",
    "            torch.save(agent.policy_network.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     env = gym.make('obstacle-v0')\n",
    "#     policy_net = PolicyNetwork(env.observation_space.shape[0], env.action_space.n, 128)\n",
    "    \n",
    "#     max_episode_num = 5000\n",
    "#     max_steps = 10000\n",
    "#     numsteps = []\n",
    "#     avg_numsteps = []\n",
    "#     all_rewards = []\n",
    "\n",
    "#     for episode in range(max_episode_num):\n",
    "#         state = env.reset()\n",
    "#         log_probs = []\n",
    "#         rewards = []\n",
    "\n",
    "#         for steps in range(max_steps):\n",
    "#             env.render()\n",
    "#             action, log_prob = policy_net.get_action(state)\n",
    "#             new_state, reward, done, _ = env.step(action)\n",
    "#             log_probs.append(log_prob)\n",
    "#             rewards.append(reward)\n",
    "\n",
    "#             if done:\n",
    "#                 update_policy(policy_net, rewards, log_probs)\n",
    "#                 numsteps.append(steps)\n",
    "#                 avg_numsteps.append(np.mean(numsteps[-10:]))\n",
    "#                 all_rewards.append(np.sum(rewards))\n",
    "#                 if episode % 1 == 0:\n",
    "#                     sys.stdout.write(\"episode: {}, total reward: {}, average_reward: {}, length: {}\\n\".format(episode, np.round(np.sum(rewards), decimals = 3),  np.round(np.mean(all_rewards[-10:]), decimals = 3), steps))\n",
    "\n",
    "#                 break\n",
    "            \n",
    "#             state = new_state\n",
    "        \n",
    "#     plt.plot(numsteps)\n",
    "#     plt.plot(avg_numsteps)\n",
    "#     plt.xlabel('Episode')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
