{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import torch  \n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class Agent():\n",
    "\n",
    "    def __init__(self, env, use_cnn=False, learning_rate=3e-4, gamma=0.99, buffer_size=10000):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.num_in = self.env.observation_space.shape[0]\n",
    "        self.num_out = self.env.action_space.n\n",
    "\n",
    "        self.ac_net = ActorCritic(self.env.observation_space.shape[0], self.env.action_space.n)\n",
    "        self.ac_optimizer = optim.Adam(self.ac_net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def update(self, rewards, values, next_value, log_probs, entropy):\n",
    "        qvals = np.zeros(len(values))\n",
    "        qval = next_value\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            qval = rewards[t] + self.gamma * qval\n",
    "            qvals[t] = qval\n",
    "        \n",
    "        values = torch.FloatTensor(values)\n",
    "        qvals = torch.FloatTensor(qvals)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        \n",
    "        advantage = qvals - values\n",
    "        actor_loss = (-log_probs * advantage).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "        ac_loss = actor_loss + 0.5 * critic_loss - 0.001 * entropy\n",
    "\n",
    "        self.ac_optimizer.zero_grad()\n",
    "        ac_loss.backward()\n",
    "        self.ac_optimizer.step()\n",
    "\n",
    "    def get_ac_output(self, state):\n",
    "        state = Variable(torch.from_numpy(state).float().unsqueeze(0))\n",
    "        value, policy_dist = self.ac_net.forward(state)\n",
    "        action = np.random.choice(self.num_out, p=policy_dist.detach().numpy().squeeze(0))\n",
    "\n",
    "        return action, policy_dist, value\n",
    "\n",
    "    def train(self, max_episode, max_step):\n",
    "        for episode in range(max_episode):\n",
    "            rewards = []\n",
    "            values = []\n",
    "            log_probs = []\n",
    "            entropy_term = 0\n",
    "            episode_reward = 0\n",
    "            \n",
    "            state = self.env.reset()\n",
    "            for steps in range(max_step):\n",
    "                action, policy_dist, value = self.get_ac_output(state)\n",
    "                new_state, reward, done, _ = self.env.step(action)  \n",
    "\n",
    "                log_prob = torch.log(policy_dist.squeeze(0)[action])\n",
    "                entropy = -torch.sum(policy_dist.mean() * torch.log(policy_dist))\n",
    "                \n",
    "                rewards.append(reward)\n",
    "                values.append(value.detach().numpy()[0])\n",
    "                log_probs.append(log_prob)\n",
    "                entropy_term += entropy\n",
    "                state = new_state\n",
    "                episode_reward += reward\n",
    "                \n",
    "                if done:\n",
    "                    if episode % 10 == 0:                    \n",
    "                        print(\"episode: \" + str(episode) + \": \" + str(episode_reward)) \n",
    "                    break\n",
    "\n",
    "            _, _, next_value = self.get_ac_output(state)\n",
    "            self.update(rewards, values, next_value, log_probs, entropy_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size=256, learning_rate=3e-4):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.critic_linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.critic_linear2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.actor_linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.actor_linear2 = nn.Linear(hidden_size, num_actions)\n",
    "    \n",
    "    def forward(self, state_tensor):\n",
    "        value = F.relu(self.critic_linear1(state_tensor))\n",
    "        value = self.critic_linear2(value)\n",
    "        \n",
    "        policy_dist = F.relu(self.actor_linear1(state_tensor))\n",
    "        policy_dist = F.softmax(self.actor_linear2(policy_dist), dim=1)\n",
    "\n",
    "        return value, policy_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0: -200.0\n",
      "episode: 10: -200.0\n",
      "episode: 20: -200.0\n",
      "episode: 30: -200.0\n",
      "episode: 40: -200.0\n",
      "episode: 50: -200.0\n",
      "episode: 60: -200.0\n",
      "episode: 70: -200.0\n",
      "episode: 80: -200.0\n",
      "episode: 90: -200.0\n",
      "episode: 100: -200.0\n",
      "episode: 110: -200.0\n",
      "episode: 120: -200.0\n",
      "episode: 130: -200.0\n",
      "episode: 140: -200.0\n",
      "episode: 150: -200.0\n",
      "episode: 160: -200.0\n",
      "episode: 170: -200.0\n",
      "episode: 180: -200.0\n",
      "episode: 190: -200.0\n",
      "episode: 200: -200.0\n",
      "episode: 210: -200.0\n",
      "episode: 220: -200.0\n",
      "episode: 230: -200.0\n",
      "episode: 240: -200.0\n",
      "episode: 250: -200.0\n",
      "episode: 260: -200.0\n",
      "episode: 270: -200.0\n",
      "episode: 280: -200.0\n",
      "episode: 290: -200.0\n",
      "episode: 300: -200.0\n",
      "episode: 310: -200.0\n",
      "episode: 320: -200.0\n",
      "episode: 330: -200.0\n",
      "episode: 340: -200.0\n",
      "episode: 350: -200.0\n",
      "episode: 360: -200.0\n",
      "episode: 370: -200.0\n",
      "episode: 380: -200.0\n",
      "episode: 390: -200.0\n",
      "episode: 400: -200.0\n",
      "episode: 410: -200.0\n",
      "episode: 420: -200.0\n",
      "episode: 430: -200.0\n",
      "episode: 440: -200.0\n",
      "episode: 450: -200.0\n",
      "episode: 460: -200.0\n",
      "episode: 470: -200.0\n",
      "episode: 480: -200.0\n",
      "episode: 490: -200.0\n",
      "episode: 500: -200.0\n",
      "episode: 510: -200.0\n",
      "episode: 520: -200.0\n",
      "episode: 530: -200.0\n",
      "episode: 540: -200.0\n",
      "episode: 550: -200.0\n",
      "episode: 560: -200.0\n",
      "episode: 570: -200.0\n",
      "episode: 580: -200.0\n",
      "episode: 590: -200.0\n",
      "episode: 600: -200.0\n",
      "episode: 610: -200.0\n",
      "episode: 620: -200.0\n",
      "episode: 630: -200.0\n",
      "episode: 640: -200.0\n",
      "episode: 650: -200.0\n",
      "episode: 660: -200.0\n",
      "episode: 670: -200.0\n",
      "episode: 680: -200.0\n",
      "episode: 690: -200.0\n",
      "episode: 700: -200.0\n",
      "episode: 710: -200.0\n",
      "episode: 720: -200.0\n",
      "episode: 730: -200.0\n",
      "episode: 740: -200.0\n",
      "episode: 750: -200.0\n",
      "episode: 760: -200.0\n",
      "episode: 770: -200.0\n",
      "episode: 780: -200.0\n",
      "episode: 790: -200.0\n",
      "episode: 800: -200.0\n",
      "episode: 810: -200.0\n",
      "episode: 820: -200.0\n",
      "episode: 830: -200.0\n",
      "episode: 840: -200.0\n",
      "episode: 850: -200.0\n",
      "episode: 860: -200.0\n",
      "episode: 870: -200.0\n",
      "episode: 880: -200.0\n",
      "episode: 890: -200.0\n",
      "episode: 900: -200.0\n",
      "episode: 910: -200.0\n",
      "episode: 920: -200.0\n",
      "episode: 930: -200.0\n",
      "episode: 940: -200.0\n",
      "episode: 950: -200.0\n",
      "episode: 960: -200.0\n",
      "episode: 970: -200.0\n",
      "episode: 980: -200.0\n",
      "episode: 990: -200.0\n",
      "episode: 1000: -200.0\n",
      "episode: 1010: -200.0\n",
      "episode: 1020: -200.0\n",
      "episode: 1030: -200.0\n",
      "episode: 1040: -200.0\n",
      "episode: 1050: -200.0\n",
      "episode: 1060: -200.0\n",
      "episode: 1070: -200.0\n",
      "episode: 1080: -200.0\n",
      "episode: 1090: -200.0\n",
      "episode: 1100: -200.0\n",
      "episode: 1110: -200.0\n",
      "episode: 1120: -200.0\n",
      "episode: 1130: -200.0\n",
      "episode: 1140: -200.0\n",
      "episode: 1150: -200.0\n",
      "episode: 1160: -200.0\n",
      "episode: 1170: -200.0\n",
      "episode: 1180: -200.0\n",
      "episode: 1190: -200.0\n",
      "episode: 1200: -200.0\n",
      "episode: 1210: -200.0\n",
      "episode: 1220: -200.0\n",
      "episode: 1230: -200.0\n",
      "episode: 1240: -200.0\n",
      "episode: 1250: -200.0\n",
      "episode: 1260: -200.0\n",
      "episode: 1270: -200.0\n",
      "episode: 1280: -200.0\n",
      "episode: 1290: -200.0\n",
      "episode: 1300: -200.0\n",
      "episode: 1310: -200.0\n",
      "episode: 1320: -200.0\n",
      "episode: 1330: -200.0\n",
      "episode: 1340: -200.0\n",
      "episode: 1350: -200.0\n",
      "episode: 1360: -200.0\n",
      "episode: 1370: -200.0\n",
      "episode: 1380: -200.0\n",
      "episode: 1390: -200.0\n",
      "episode: 1400: -200.0\n",
      "episode: 1410: -200.0\n",
      "episode: 1420: -200.0\n",
      "episode: 1430: -200.0\n",
      "episode: 1440: -200.0\n",
      "episode: 1450: -200.0\n",
      "episode: 1460: -200.0\n",
      "episode: 1470: -200.0\n",
      "episode: 1480: -200.0\n",
      "episode: 1490: -200.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "env_id = 'MountainCar-v0'#\"CartPole-v0\"\n",
    "env = gym.make(env_id)\n",
    "agent = Agent(env)\n",
    "agent.train(1500, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
